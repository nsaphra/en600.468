#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
from sklearn import svm
 
def word_matches(h, ref):
    return sum(1 for w in h if w in ref)

def distance(h, r):
    dist_map = {}

    def dist(len_h, len_r):
        if len_h == 0:
            return len_r
        if len_r == 0:
            return len_h
        if (len_h, len_r) in dist_map:
            return dist_map[(len_h, len_r)]
        cost = 1
        if h[len_h-1] == r[len_r-1]:
            cost = 0
        ret = min(dist(len_h-1, len_r) +1,
            dist(len_h, len_r-1) +1,
            dist(len_h - 1, len_r-1) + cost)
        dist_map[(len_h, len_r)] = ret
        return ret
    return dist(len(h), len(r))


def get_ngrams(h):
    hset_1gram = set()
    hset_2gram = set()
    hset_3gram = set()
    hset_4gram = set()
    for (ind, w) in enumerate(h):
        hset_1gram.add(w)
        if ind > 0:
            hset_2gram.add(' '.join([h[ind-1], w]))
            if ind > 1:
                hset_3gram.add(' '.join([h[ind-2], h[ind-1], w]))
                if ind > 2:
                    hset_4gram.add(' '.join([h[ind-3], h[ind-2], h[ind-1], w]))
    return (hset_1gram, hset_2gram, hset_3gram, hset_4gram)

def get_gramfeats(hset, rset):
    gramfeats = []
    gramfeats += [len(hset & rset)] # raw match data
#    if rset:
#        gramfeats += [len(hset & rset)/len(rset)] # recall
#    else:
#        gramfeats += [0]
#    if hset:
#        gramfeats += [len(hset & rset)/len(hset)] # precision
#    else:
#        gramfeats += [0]
    gramfeats += [len(hset)]
    return gramfeats

def get_sentfeats(h, ref):
    ngram_hsets = get_ngrams(h)
    ngram_rsets = get_ngrams(ref)
    hfeats = []
    for (hset, rset) in zip(ngram_hsets, ngram_rsets):
        hfeats += get_gramfeats(hset, rset)
    hfeats += [len(h), len(h)/len(ref)] # brevity penalties
    hfeats += [min(1, len(h)/len(ref))] # bleu brevity penalty
    hfeats += [distance(h, ref)] # levenshtein dist
    return hfeats
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    parser.add_argument('-l', '--labels', default='data/dev.answers',
            help='dev labels for training linear classifier')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]

    labels = [int(l) for l in open(opts.labels)]
 
    h_feats = []
    h1s = []
    h2s = []
    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        rset = set(ref)

        h1_feats = get_sentfeats(h1, ref)
        h2_feats = get_sentfeats(h2, ref)
        h1s.append(h1)
        h2s.append(h2)

        h_feats.append(h1_feats + h2_feats)

    svc = svm.SVC()
    svc.fit(h_feats[:len(labels)], labels)
    results = svc.predict(h_feats)
    for (i, score) in enumerate(results):
        if h1s[i] == h2s[i]:
            print 0
        else:
            print score

 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()

    
